{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lhotse.dataset.input_strategies import (  # noqa F401 For AudioSamples\n",
    "    AudioSamples,\n",
    "    OnTheFlyFeatures,\n",
    ")\n",
    "\n",
    "from lhotse import CutSet, Fbank, FbankConfig, load_manifest, load_manifest_lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lhotse.dataset.input_strategies.OnTheFlyFeatures at 0x78ec3021fbb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OnTheFlyFeatures(Fbank(FbankConfig(num_mel_bins=80)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_keys = ['best_train_loss',\n",
    " 'best_valid_loss',\n",
    " 'best_train_epoch',\n",
    " 'best_valid_epoch',\n",
    " 'batch_idx_train',\n",
    " 'log_interval',\n",
    " 'reset_interval',\n",
    " 'valid_interval',\n",
    " 'feature_dim',\n",
    " 'subsampling_factor',\n",
    " 'warm_step',\n",
    " 'env_info',\n",
    " 'audio_file',\n",
    " 'epoch',\n",
    " 'iter',\n",
    " 'avg',\n",
    " 'use_averaged_model',\n",
    " 'exp_dir',\n",
    " 'bpe_model',\n",
    " 'lang_dir',\n",
    " 'decoding_method',\n",
    " 'beam_size',\n",
    " 'beam',\n",
    " 'ngram_lm_scale',\n",
    " 'max_contexts',\n",
    " 'max_states',\n",
    " 'context_size',\n",
    " 'max_sym_per_frame',\n",
    " 'num_paths',\n",
    " 'nbest_scale',\n",
    " 'use_shallow_fusion',\n",
    " 'lm_type',\n",
    " 'lm_scale',\n",
    " 'tokens_ngram',\n",
    " 'backoff_id',\n",
    " 'num_encoder_layers',\n",
    " 'feedforward_dims',\n",
    " 'nhead',\n",
    " 'encoder_dims',\n",
    " 'attention_dims',\n",
    " 'encoder_unmasked_dims',\n",
    " 'zipformer_downsampling_factors',\n",
    " 'cnn_module_kernels',\n",
    " 'decoder_dim',\n",
    " 'joiner_dim',\n",
    " 'short_chunk_size',\n",
    " 'num_left_chunks',\n",
    " 'decode_chunk_len',\n",
    " 'full_libri',\n",
    " 'mini_libri',\n",
    " 'manifest_dir',\n",
    " 'max_duration',\n",
    " 'bucketing_sampler',\n",
    " 'num_buckets',\n",
    " 'concatenate_cuts',\n",
    " 'duration_factor',\n",
    " 'gap',\n",
    " 'on_the_fly_feats',\n",
    " 'shuffle',\n",
    " 'drop_last',\n",
    " 'return_cuts',\n",
    " 'num_workers',\n",
    " 'enable_spec_aug',\n",
    " 'spec_aug_time_warp_factor',\n",
    " 'enable_musan',\n",
    " 'input_strategy',\n",
    " 'lm_vocab_size',\n",
    " 'lm_epoch',\n",
    " 'lm_avg',\n",
    " 'lm_exp_dir',\n",
    " 'rnn_lm_embedding_dim',\n",
    " 'rnn_lm_hidden_dim',\n",
    " 'rnn_lm_num_layers',\n",
    " 'rnn_lm_tie_weights',\n",
    " 'transformer_lm_exp_dir',\n",
    " 'transformer_lm_dim_feedforward',\n",
    " 'transformer_lm_encoder_dim',\n",
    " 'transformer_lm_embedding_dim',\n",
    " 'transformer_lm_nhead',\n",
    " 'transformer_lm_num_layers',\n",
    " 'transformer_lm_tie_weights',\n",
    " 'res_dir',\n",
    " 'suffix',\n",
    " 'blank_id',\n",
    " 'unk_id',\n",
    " 'vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "params_dict = {'attention_dims': '192,192,192,192,192',\n",
    " 'audio_file': '/root/datadrive/TEDxJP-10K_v1.1/wav/VM7Zk_J6gfQ.16k.wav',\n",
    " 'avg': 1,\n",
    " 'backoff_id': 500,\n",
    " 'batch_idx_train': 0,\n",
    " 'beam': 20.0,\n",
    " 'beam_size': 4,\n",
    " 'best_train_epoch': -1,\n",
    " 'best_train_loss': np.inf,\n",
    " 'best_valid_epoch': -1,\n",
    " 'best_valid_loss': np.inf,\n",
    " 'blank_id': 0,\n",
    " 'bpe_model': 'exp/bpe.model',\n",
    " 'bucketing_sampler': True,\n",
    " 'cnn_module_kernels': '31,31,31,31,31',\n",
    " 'concatenate_cuts': False,\n",
    " 'context_size': 2,\n",
    " 'decode_chunk_len': 32,\n",
    " 'decoder_dim': 512,\n",
    " 'decoding_method': 'greedy_search',\n",
    " 'drop_last': True,\n",
    " 'duration_factor': 1.0,\n",
    " 'enable_musan': True,\n",
    " 'enable_spec_aug': True,\n",
    " 'encoder_dims': '384,384,384,384,384',\n",
    " 'encoder_unmasked_dims': '256,256,256,256,256',\n",
    " 'env_info': {'IP address': '172.20.0.2',\n",
    "              'hostname': '3ca8273189ad',\n",
    "              'icefall-git-branch': 'my-env',\n",
    "              'icefall-git-date': 'Sat Jun 8 08:52:43 2024',\n",
    "              'icefall-git-sha1': 'b75befc8-dirty',\n",
    "              'icefall-path': '/workspace/icefall',\n",
    "              'k2-build-type': 'Release',\n",
    "              'k2-git-date': 'Thu Jun 6 02:13:08 2024',\n",
    "              'k2-git-sha1': '8f976a1e1407e330e2a233d68f81b1eb5269fdaa',\n",
    "              'k2-path': '/opt/conda/lib/python3.10/site-packages/k2/__init__.py',\n",
    "              'k2-version': '1.24.4',\n",
    "              'k2-with-cuda': True,\n",
    "              'lhotse-path': '/opt/conda/lib/python3.10/site-packages/lhotse/__init__.py',\n",
    "              'lhotse-version': '1.24.0.dev+git.4d57d53.clean',\n",
    "              'python-version': '3.10',\n",
    "              'torch-cuda-available': True,\n",
    "              'torch-cuda-version': '12.1',\n",
    "              'torch-version': '2.3.1'},\n",
    " 'epoch': 30,\n",
    " 'exp_dir': 'exp',\n",
    " 'feature_dim': 80,\n",
    " 'feedforward_dims': '1024,1024,2048,2048,1024',\n",
    " 'full_libri': True,\n",
    " 'gap': 1.0,\n",
    " 'input_strategy': 'PrecomputedFeatures',\n",
    " 'iter': 0,\n",
    " 'joiner_dim': 512,\n",
    " 'lang_dir': 'data/lang_bpe_500',\n",
    " 'lm_avg': 1,\n",
    " 'lm_epoch': 7,\n",
    " 'lm_exp_dir': None,\n",
    " 'lm_scale': 0.3,\n",
    " 'lm_type': 'rnn',\n",
    " 'lm_vocab_size': 500,\n",
    " 'log_interval': 50,\n",
    " 'manifest_dir': 'data/fbank',\n",
    " 'max_contexts': 8,\n",
    " 'max_duration': 200.0,\n",
    " 'max_states': 64,\n",
    " 'max_sym_per_frame': 1,\n",
    " 'mini_libri': False,\n",
    " 'nbest_scale': 0.5,\n",
    " 'ngram_lm_scale': 0.01,\n",
    " 'nhead': '8,8,8,8,8',\n",
    " 'num_buckets': 30,\n",
    " 'num_encoder_layers': '2,4,3,2,4',\n",
    " 'num_left_chunks': 4,\n",
    " 'num_paths': 200,\n",
    " 'num_workers': 2,\n",
    " 'on_the_fly_feats': False,\n",
    " 'res_dir': 'exp/greedy_search',\n",
    " 'reset_interval': 200,\n",
    " 'return_cuts': True,\n",
    " 'rnn_lm_embedding_dim': 2048,\n",
    " 'rnn_lm_hidden_dim': 2048,\n",
    " 'rnn_lm_num_layers': 3,\n",
    " 'rnn_lm_tie_weights': True,\n",
    " 'short_chunk_size': 50,\n",
    " 'shuffle': True,\n",
    " 'spec_aug_time_warp_factor': 80,\n",
    " 'subsampling_factor': 4,\n",
    " 'suffix': 'epoch-30-avg-1-streaming-chunk-size-32-context-2-max-sym-per-frame-1-use-averaged-model',\n",
    " 'tokens_ngram': 2,\n",
    " 'transformer_lm_dim_feedforward': 2048,\n",
    " 'transformer_lm_embedding_dim': 768,\n",
    " 'transformer_lm_encoder_dim': 768,\n",
    " 'transformer_lm_exp_dir': None,\n",
    " 'transformer_lm_nhead': 8,\n",
    " 'transformer_lm_num_layers': 16,\n",
    " 'transformer_lm_tie_weights': True,\n",
    " 'unk_id': 2,\n",
    " 'use_averaged_model': True,\n",
    " 'use_shallow_fusion': False,\n",
    " 'valid_interval': 3000,\n",
    " 'vocab_size': 2500,\n",
    " 'warm_step': 2000,\n",
    " 'zipformer_downsampling_factors': '1,2,4,8,2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('params.json', 'w') as f:\n",
    "    json.dump(params_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lhotse import (\n",
    "    RecordingSet,\n",
    "    CutSet,\n",
    ")\n",
    "from lhotse.cut import Cut\n",
    "from pathlib import Path\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_recording_set(audio_file):\n",
    "    recording_id = Path(audio_file).stem\n",
    "    audio, sample_rate = torchaudio.load(audio_file)\n",
    "    duration = audio.size(1) / sample_rate  # get duration in seconds\n",
    "\n",
    "    from lhotse import Recording\n",
    "    sources = AttributeDict({\n",
    "        \"type\": 'file',\n",
    "        \"channels\": [0],\n",
    "        \"source\": audio_file,\n",
    "        \"has_video\": False,\n",
    "    })\n",
    "    recording = Recording.from_file(audio_file)\n",
    "\n",
    "    return RecordingSet.from_recordings([recording])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cut_set_and_split(recording_set, window_duration: float = 0.1):\n",
    "    cuts = CutSet.from_manifests(recordings=recording_set)\n",
    "    split_cuts = cuts.cut_into_windows(duration=window_duration)\n",
    "    return split_cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = '/root/datadrive/TEDxJP-10K_v1.1/wav/VM7Zk_J6gfQ.16k.wav'\n",
    "recording_set = create_recording_set(audio_file)\n",
    "cut_set = create_cut_set_and_split(recording_set, window_duration=0.1)\n",
    "cut_set.to_file('cut_set.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lhotse.cut.mono.MonoCut"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(next(iter(cut_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__dataclass_fields__',\n",
       " '__dataclass_params__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__match_args__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " 'append',\n",
       " 'attach_tensor',\n",
       " 'channel',\n",
       " 'compute_and_store_features',\n",
       " 'compute_features',\n",
       " 'custom',\n",
       " 'cut_into_windows',\n",
       " 'dereverb_wpe',\n",
       " 'drop_alignments',\n",
       " 'drop_custom',\n",
       " 'drop_features',\n",
       " 'drop_recording',\n",
       " 'drop_supervisions',\n",
       " 'duration',\n",
       " 'end',\n",
       " 'extend_by',\n",
       " 'features',\n",
       " 'features_type',\n",
       " 'fill_supervision',\n",
       " 'filter_supervisions',\n",
       " 'frame_shift',\n",
       " 'from_dict',\n",
       " 'has',\n",
       " 'has_custom',\n",
       " 'has_features',\n",
       " 'has_overlapping_supervisions',\n",
       " 'has_recording',\n",
       " 'has_video',\n",
       " 'id',\n",
       " 'index_supervisions',\n",
       " 'load_audio',\n",
       " 'load_custom',\n",
       " 'load_features',\n",
       " 'load_video',\n",
       " 'map_supervisions',\n",
       " 'merge_supervisions',\n",
       " 'mix',\n",
       " 'move_to_memory',\n",
       " 'normalize_loudness',\n",
       " 'num_channels',\n",
       " 'num_features',\n",
       " 'num_frames',\n",
       " 'num_samples',\n",
       " 'pad',\n",
       " 'perturb_speed',\n",
       " 'perturb_tempo',\n",
       " 'perturb_volume',\n",
       " 'play_audio',\n",
       " 'plot_alignment',\n",
       " 'plot_audio',\n",
       " 'plot_features',\n",
       " 'recording',\n",
       " 'recording_id',\n",
       " 'resample',\n",
       " 'reverb_rir',\n",
       " 'sampling_rate',\n",
       " 'save_audio',\n",
       " 'speakers_audio_mask',\n",
       " 'speakers_feature_mask',\n",
       " 'split',\n",
       " 'start',\n",
       " 'supervisions',\n",
       " 'supervisions_audio_mask',\n",
       " 'supervisions_feature_mask',\n",
       " 'to_dict',\n",
       " 'trim_to_alignments',\n",
       " 'trim_to_supervision_groups',\n",
       " 'trim_to_supervisions',\n",
       " 'trimmed_supervisions',\n",
       " 'truncate',\n",
       " 'video',\n",
       " 'with_channels',\n",
       " 'with_custom',\n",
       " 'with_features_path_prefix',\n",
       " 'with_id',\n",
       " 'with_recording_path_prefix']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut = next(iter(cut_set))\n",
    "dir(cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.2/162.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.53.0 kiwisolver-1.4.5 matplotlib-3.9.0 pyparsing-3.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = '/root/datadrive/TEDxJP-10K_v1.1/wav/VM7Zk_J6gfQ.16k.wav'\n",
    "recording_set = create_recording_set(audio_file)\n",
    "short_cut_set = create_cut_set_and_split(recording_set, window_duration=0.1)\n",
    "long_cut_set = create_cut_set_and_split(recording_set, window_duration=10)\n",
    "\n",
    "short_cut = next(iter(short_cut_set))\n",
    "long_cut = next(iter(long_cut_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0671844e-06\n",
      "2.479553e-07\n"
     ]
    }
   ],
   "source": [
    "print(long_cut.load_audio()[:1600].mean())\n",
    "print(short_cut.load_audio().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icefall.utils import AttributeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'file',\n",
       " 'channels': [0],\n",
       " 'source': '/root/datadrive/TEDxJP-10K_v1.1/wav/VM7Zk_J6gfQ.16k.wav'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AttributeDict({\n",
    "                \"type\": 'file',\n",
    "                \"channels\": [0],\n",
    "                \"source\": audio_file,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streaming_decode import decode_one_chunk, decode_dataset\n",
    "from train import get_transducer_model\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from icefall.utils import AttributeDict\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "params_dict = AttributeDict(params_dict)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(params_dict.bpe_model)\n",
    "model = get_transducer_model(params_dict)\n",
    "model.load_state_dict(torch.load('exp/epoch-30.pt')['model'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import k2\n",
    "from decode_stream import DecodeStream\n",
    "from kaldifeat import Fbank, FbankOptions\n",
    "import logging\n",
    "\n",
    "def decode_stream(\n",
    "    cuts: CutSet,\n",
    "    params: AttributeDict,\n",
    "    model: nn.Module,\n",
    "    sp: spm.SentencePieceProcessor,\n",
    "    decoding_graph: Optional[k2.Fsa] = None,\n",
    ") -> Dict[str, List[Tuple[List[str], List[str]]]]:\n",
    "    \"\"\"Decode dataset.\n",
    "\n",
    "    Args:\n",
    "      cuts:\n",
    "        Lhotse Cutset containing the dataset to decode.\n",
    "      params:\n",
    "        It is returned by :func:`get_params`.\n",
    "      model:\n",
    "        The neural model.\n",
    "      sp:\n",
    "        The BPE model.\n",
    "      decoding_graph:\n",
    "        The decoding graph. Can be either a `k2.trivial_graph` or HLG, Used\n",
    "        only when --decoding_method is fast_beam_search.\n",
    "    Returns:\n",
    "      Return a dict, whose key may be \"greedy_search\" if greedy search\n",
    "      is used, or it may be \"beam_7\" if beam size of 7 is used.\n",
    "      Its value is a list of tuples. Each tuple contains two elements:\n",
    "      The first is the reference transcript, and the second is the\n",
    "      predicted result.\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "\n",
    "    opts = FbankOptions()\n",
    "    opts.device = device\n",
    "    opts.frame_opts.dither = 0\n",
    "    opts.frame_opts.snip_edges = False\n",
    "    opts.frame_opts.samp_freq = 16000\n",
    "    opts.mel_opts.num_bins = 80\n",
    "    opts.mel_opts.high_freq = -400\n",
    "\n",
    "    log_interval = 50\n",
    "\n",
    "    decode_results = []\n",
    "    # Contain decode streams currently running.\n",
    "    decode_streams = []\n",
    "    for num, cut in enumerate(cuts):\n",
    "        # each utterance has a DecodeStream.\n",
    "        initial_states = model.encoder.get_init_state(device=device)\n",
    "        decode_stream = DecodeStream(\n",
    "            params=params,\n",
    "            cut_id=cut.id,\n",
    "            initial_states=initial_states,\n",
    "            decoding_graph=decoding_graph,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        audio: np.ndarray = cut.load_audio()\n",
    "        # audio.shape: (1, num_samples)\n",
    "        assert len(audio.shape) == 2\n",
    "        assert audio.shape[0] == 1, \"Should be single channel\"\n",
    "        assert audio.dtype == np.float32, audio.dtype\n",
    "\n",
    "        # The trained model is using normalized samples\n",
    "        # - this is to avoid sending [-32k,+32k] signal in...\n",
    "        # - some lhotse AudioTransform classes can make the signal\n",
    "        #   be out of range [-1, 1], hence the tolerance 10\n",
    "        assert (\n",
    "            np.abs(audio).max() <= 10\n",
    "        ), \"Should be normalized to [-1, 1], 10 for tolerance...\"\n",
    "\n",
    "        samples = torch.from_numpy(audio).squeeze(0)\n",
    "\n",
    "        fbank = Fbank(opts)\n",
    "        feature = fbank(samples.to(device))\n",
    "        decode_stream.set_features(feature, tail_pad_len=params.decode_chunk_len)\n",
    "\n",
    "        decode_streams.append(decode_stream)\n",
    "\n",
    "        while len(decode_streams) >= params.num_decode_streams:\n",
    "            finished_streams = decode_one_chunk(\n",
    "                params=params, model=model, decode_streams=decode_streams\n",
    "            )\n",
    "            for i in sorted(finished_streams, reverse=True):\n",
    "                decode_results.append(\n",
    "                    (\n",
    "                        decode_streams[i].id,\n",
    "                        sp.decode(decode_streams[i].decoding_result()).split(),\n",
    "                    )\n",
    "                )\n",
    "                del decode_streams[i]\n",
    "\n",
    "        if num % log_interval == 0:\n",
    "            logging.info(f\"Cuts processed until now is {num}.\")\n",
    "\n",
    "    # decode final chunks of last sequences\n",
    "    while len(decode_streams):\n",
    "        finished_streams = decode_one_chunk(\n",
    "            params=params, model=model, decode_streams=decode_streams\n",
    "        )\n",
    "        for i in sorted(finished_streams, reverse=True):\n",
    "            decode_results.append(\n",
    "                (\n",
    "                    decode_streams[i].id,\n",
    "                    decode_streams[i].ground_truth.split(),\n",
    "                    sp.decode(decode_streams[i].decoding_result()).split(),\n",
    "                )\n",
    "            )\n",
    "            del decode_streams[i]\n",
    "\n",
    "    if params.decoding_method == \"greedy_search\":\n",
    "        key = \"greedy_search\"\n",
    "    elif params.decoding_method == \"fast_beam_search\":\n",
    "        key = (\n",
    "            f\"beam_{params.beam}_\"\n",
    "            f\"max_contexts_{params.max_contexts}_\"\n",
    "            f\"max_states_{params.max_states}\"\n",
    "        )\n",
    "    elif params.decoding_method == \"modified_beam_search\":\n",
    "        key = f\"num_active_paths_{params.num_active_paths}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported decoding method: {params.decoding_method}\")\n",
    "    return {key: decode_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m params_dict\u001b[38;5;241m.\u001b[39mnum_decode_streams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdecode_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshort_cut_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 85\u001b[0m, in \u001b[0;36mdecode_stream\u001b[0;34m(cuts, params, model, sp, decoding_graph)\u001b[0m\n\u001b[1;32m     82\u001b[0m decode_streams\u001b[38;5;241m.\u001b[39mappend(decode_stream)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(decode_streams) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mnum_decode_streams:\n\u001b[0;32m---> 85\u001b[0m     finished_streams \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_one_chunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_streams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_streams\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(finished_streams, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     89\u001b[0m         decode_results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     90\u001b[0m             (\n\u001b[1;32m     91\u001b[0m                 decode_streams[i]\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m     92\u001b[0m                 sp\u001b[38;5;241m.\u001b[39mdecode(decode_streams[i]\u001b[38;5;241m.\u001b[39mdecoding_result())\u001b[38;5;241m.\u001b[39msplit(),\n\u001b[1;32m     93\u001b[0m             )\n\u001b[1;32m     94\u001b[0m         )\n",
      "File \u001b[0;32m~/workspace/examples/streaming-asr/streaming_decode.py:244\u001b[0m, in \u001b[0;36mdecode_one_chunk\u001b[0;34m(params, model, decode_streams)\u001b[0m\n\u001b[1;32m    241\u001b[0m states \u001b[38;5;241m=\u001b[39m stack_states(states)\n\u001b[1;32m    242\u001b[0m processed_lens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(processed_lens, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 244\u001b[0m encoder_out, encoder_out_lens, new_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreaming_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_lens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_lens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m encoder_out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mjoiner\u001b[38;5;241m.\u001b[39mencoder_proj(encoder_out)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params\u001b[38;5;241m.\u001b[39mdecoding_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreedy_search\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/workspace/examples/streaming-asr/zipformer.py:634\u001b[0m, in \u001b[0;36mZipformer.streaming_forward\u001b[0;34m(self, x, x_lens, states)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    633\u001b[0m     x \u001b[38;5;241m=\u001b[39m skip_module(outputs[k], x)\n\u001b[0;32m--> 634\u001b[0m x, len_avg, avg, key, val, val2, conv1, conv2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreaming_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_len\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_avg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_avg\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_key\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_val2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_val2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_conv1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_conv1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_conv2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_conv2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m outputs\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# Update caches\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/examples/streaming-asr/zipformer.py:1490\u001b[0m, in \u001b[0;36mDownsampledZipformerEncoder.streaming_forward\u001b[0;34m(self, src, cached_len, cached_avg, cached_key, cached_val, cached_val2, cached_conv1, cached_conv2)\u001b[0m\n\u001b[1;32m   1478\u001b[0m src_orig \u001b[38;5;241m=\u001b[39m src\n\u001b[1;32m   1479\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(src)\n\u001b[1;32m   1481\u001b[0m (\n\u001b[1;32m   1482\u001b[0m     src,\n\u001b[1;32m   1483\u001b[0m     cached_len,\n\u001b[1;32m   1484\u001b[0m     cached_avg,\n\u001b[1;32m   1485\u001b[0m     cached_key,\n\u001b[1;32m   1486\u001b[0m     cached_val,\n\u001b[1;32m   1487\u001b[0m     cached_val2,\n\u001b[1;32m   1488\u001b[0m     cached_conv1,\n\u001b[1;32m   1489\u001b[0m     cached_conv2,\n\u001b[0;32m-> 1490\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreaming_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_avg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_avg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_val2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_val2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_conv1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_conv1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached_conv2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_conv2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1500\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(src)\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# remove any extra frames that are not a multiple of downsample_factor\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/examples/streaming-asr/zipformer.py:1338\u001b[0m, in \u001b[0;36mZipformerEncoder.streaming_forward\u001b[0;34m(self, src, cached_len, cached_avg, cached_key, cached_val, cached_val2, cached_conv1, cached_conv2)\u001b[0m\n\u001b[1;32m   1336\u001b[0m new_cached_conv2 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m-> 1338\u001b[0m     output, len_avg, avg, key, val, val2, conv1, conv2 \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreaming_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcached_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_len\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcached_avg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_avg\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcached_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_key\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcached_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcached_val2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_val2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcached_conv1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_conv1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcached_conv2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_conv2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;66;03m# Update caches\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m     new_cached_len\u001b[38;5;241m.\u001b[39mappend(len_avg)\n",
      "File \u001b[0;32m~/workspace/examples/streaming-asr/zipformer.py:1040\u001b[0m, in \u001b[0;36mZipformerEncoderLayer.streaming_forward\u001b[0;34m(self, src, pos_emb, cached_len, cached_avg, cached_key, cached_val, cached_val2, cached_conv1, cached_conv2)\u001b[0m\n\u001b[1;32m   1034\u001b[0m src_conv, cached_conv1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_module1\u001b[38;5;241m.\u001b[39mstreaming_forward(\n\u001b[1;32m   1035\u001b[0m     src,\n\u001b[1;32m   1036\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcached_conv1,\n\u001b[1;32m   1037\u001b[0m )\n\u001b[1;32m   1038\u001b[0m src \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m+\u001b[39m src_conv\n\u001b[0;32m-> 1040\u001b[0m src \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward2\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m src_attn, cached_val2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mstreaming_forward2(\n\u001b[1;32m   1043\u001b[0m     src,\n\u001b[1;32m   1044\u001b[0m     attn_weights,\n\u001b[1;32m   1045\u001b[0m     cached_val\u001b[38;5;241m=\u001b[39mcached_val2,\n\u001b[1;32m   1046\u001b[0m )\n\u001b[1;32m   1047\u001b[0m src \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m+\u001b[39m src_attn\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/examples/streaming-asr/zipformer.py:2541\u001b[0m, in \u001b[0;36mFeedforwardModule.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2539\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj(x)\n\u001b[1;32m   2540\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbalancer(x)\n\u001b[0;32m-> 2541\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2542\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m   2543\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/examples/streaming-asr/scaling.py:1020\u001b[0m, in \u001b[0;36mDoubleSwish.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(x \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDoubleSwishFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m~/workspace/examples/streaming-asr/scaling.py:981\u001b[0m, in \u001b[0;36mDoubleSwishFunction.forward\u001b[0;34m(ctx, x)\u001b[0m\n\u001b[1;32m    978\u001b[0m y \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m s\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m requires_grad:\n\u001b[0;32m--> 981\u001b[0m     deriv \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m) \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;66;03m# notes on derivative of x * sigmoid(x - 1):\u001b[39;00m\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;66;03m# https://www.wolframalpha.com/input?i=d%2Fdx+%28x+*+sigmoid%28x-1%29%29\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;66;03m# min \\simeq -0.043638.  Take floor as -0.043637 so it's a lower bund\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# max \\simeq 1.1990.   Take ceil to be 1.2 so it's an upper bound.\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# the combination of \"+ torch.rand_like(deriv)\" and casting to torch.uint8 (which\u001b[39;00m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;66;03m# floors), should be expectation-preserving.\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     floor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.043637\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:966\u001b[0m, in \u001b[0;36mTensor.__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rsub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VariableFunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "params_dict.num_decode_streams=1000\n",
    "decode_stream(short_cut_set, params_dict, model, sp=sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.215391"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters()]) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
